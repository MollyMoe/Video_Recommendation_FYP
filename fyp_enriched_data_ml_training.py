# -*- coding: utf-8 -*-
"""fyp-enriched-data-ml-training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/Uf4i/a33a200a8043d02c031ed21844378962/fyp-enriched-data-ml-training.ipynb
"""

# Install PySpark
!pip install -q pyspark

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("HybridRecommender") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()

print(" Spark session started successfully!")

spark.range(5).show()

from google.colab import drive
drive.mount('/content/drive')

ratings_path = "/content/drive/MyDrive/MovieLens/ratings.csv"
ratings_df = spark.read.csv(ratings_path, header=True, inferSchema=True)

ratings_df.show(5)
ratings_df.printSchema()

"""Als collabrative filtering model"""

from pyspark.ml.recommendation import ALS

als = ALS(
    userCol="userId",
    itemCol="movieId",
    ratingCol="rating",
    coldStartStrategy="drop",
    nonnegative=True,
    implicitPrefs=False,
    maxIter=10,
    regParam=0.1,
    rank=10
)

model = als.fit(ratings_df)

# Recommend top 5 movies per user
user_recs = model.recommendForAllUsers(5)
user_recs.show(5, truncate=False)

"""convert als to pandas"""

user_recs_pd = user_recs.toPandas()

import pandas as pd

# Flatten ALS output
flattened_recs = []

for _, row in user_recs_pd.iterrows():
    user_id = row['userId']
    for rec in row['recommendations']:
        flattened_recs.append({
            'userId': user_id,
            'movieId': rec['movieId'],
            'predicted_rating': rec['rating']
        })

als_flat_df = pd.DataFrame(flattened_recs)
als_flat_df.head()

movies_path = "/content/drive/MyDrive/MovieLens/movies_tmdbMetadata.csv"
movies_df = pd.read_csv(movies_path)

print(movies_df.columns)

"""merge als + movie metada"""

hybrid_df = als_flat_df.merge(movies_df, on="movieId", how="left")
hybrid_df.head()

hybrid_df.to_csv("/content/drive/MyDrive/MovieLens/hybrid_recommendations.csv", index=False)
print("Hybrid recommendations saved to Drive.")